% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  a4paper,
]{article}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{vntex} % Gói hỗ trợ tiếng Việt đầy đủ
  \usepackage{amsmath} % Hỗ trợ các ký hiệu Toán học
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\ifLuaTeX
\usepackage[bidi=basic]{babel}
\else
\usepackage[bidi=default]{babel}
\fi
\babelprovide[main,import]{vietnamese}
% get rid of language-specific shorthands (see #6817):
\let\LanguageShortHands\languageshorthands
\def\languageshorthands#1{}
\usepackage{setspace}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{setspace}
\onehalfspacing
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Đồ án -- Phân lớp},
  pdflang={vi},
  colorlinks=true,
  linkcolor={Maroon},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={red},
  pdfcreator={LaTeX via pandoc}}

\title{Đồ án -- Phân lớp}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Multivariate Statistical Applied}
\author{}
\date{\today}


\begin{document}
\maketitle
\begin{abstract}
Phân loại (Classification) là một trong những bài toán quan trọng trong
học máy (Machine Learning) và thống kê, với mục tiêu phân nhóm các đối
tượng dựa trên các đặc trưng (features) đã biết. Trong đó, Phân tích
phân biệt tuyến tính (Linear Discriminant Analysis - LDA) là một phương
pháp phổ biến, giúp tìm kiếm một không gian chiếu tối ưu để phân tách
các nhóm dữ liệu. LDA không chỉ giúp cải thiện khả năng phân loại mà còn
hỗ trợ giảm chiều dữ liệu, giúp tối ưu hóa hiệu suất tính toán và tránh
hiện tượng quá khớp (overfitting).

Trong bài toán phân loại hai lớp, LDA có thể được hiểu như Fisher's
Linear Discriminant (FLD), trong đó mục tiêu là tìm một vector chiếu tối
ưu để tối đa hóa độ phân biệt giữa hai lớp bằng cách tối đa hóa phương
sai giữa các lớp và tối thiểu hóa phương sai trong từng lớp. Tuy nhiên,
LDA không chỉ giới hạn trong bài toán hai lớp mà còn có thể mở rộng để
phân biệt nhiều lớp bằng cách tìm tập hợp các vector chiếu, giúp tối ưu
hóa khả năng phân tách giữa nhiều nhóm dữ liệu trong không gian đặc
trưng.

Trong nghiên cứu này, chúng tôi tiến hành thực nghiệm với một case study
thực tế, áp dụng LDA trên các tập dữ liệu có nhãn để đánh giá hiệu suất
của phương pháp này. Chúng tôi sử dụng các chỉ số đánh giá như độ chính
xác (Accuracy), độ nhạy (Sensitivity), độ đặc hiệu (Specificity) và
F1-score để đo lường khả năng phân loại của mô hình. Kết quả thực nghiệm
cho thấy rằng LDA hoạt động tốt khi dữ liệu có phân phối chuẩn và ma
trận hiệp phương sai đồng nhất giữa các lớp, nhưng có thể bị suy giảm
hiệu suất khi các giả định này không được thỏa mãn.

Bên cạnh đánh giá lý thuyết và thực nghiệm, chúng tôi cũng phân tích các
ứng dụng thực tế của LDA trong nhiều lĩnh vực khác nhau như nhận dạng
khuôn mặt, chẩn đoán y khoa, tài chính và sinh học. Với khả năng giảm
chiều dữ liệu hiệu quả trong khi vẫn bảo toàn thông tin quan trọng, LDA
trở thành một công cụ mạnh mẽ trong việc trích xuất đặc trưng và phân
loại dữ liệu có cấu trúc phức tạp.

Những phát hiện từ nghiên cứu này giúp cung cấp một cái nhìn toàn diện
về vai trò của LDA trong bài toán phân loại, từ bài toán phân biệt hai
lớp đơn giản đến bài toán phân biệt nhiều lớp, cùng với các ứng dụng
thực tế của nó.

Từ khóa: Phân loại (Classification), Phân tích phân biệt tuyến tính
(LDA), Fisher's Linear Discriminant (FLD), Học máy (Machine Learning),
Giảm chiều dữ liệu (Dimensionality Reduction), Ứng dụng LDA.
\end{abstract}

\pagebreak
\tableofcontents 
\pagebreak

\section{Giới thiệu}\label{giux1edbi-thiux1ec7u}

\subsection{Phân biệt (Discrimination) và Phân lớp
(Classification)}\label{phuxe2n-biux1ec7t-discrimination-vuxe0-phuxe2n-lux1edbp-classification}

\subsection{Ý nghĩa về khoa
học}\label{uxfd-nghux129a-vux1ec1-khoa-hux1ecdc}

\subsection{Ý nghĩa về ứng
dụng}\label{uxfd-nghux129a-vux1ec1-ux1ee9ng-dux1ee5ng}

\subsection{Phát biểu bài toán}\label{phuxe1t-biux1ec3u-buxe0i-touxe1n}

\begin{itemize}
\item
  \textbf{Giả sử:}

  \begin{itemize}
  \tightlist
  \item
    Có \(g\) nhóm khác nhau, ký hiệu \(G = \{G_1, G_2, \ldots, G_g\}\).
    Mỗi nhóm có một phân phối xác suất riêng với trung bình \(\mu_G\) và
    ma trận hiệp phương sai \(\Sigma_G\).
  \item
    Một quan sát \(x\) có \(d\) đặc trưng và thuộc về một trong các nhóm
    \(G\).
  \end{itemize}
\item
  \textbf{Đầu vào:}

  \begin{itemize}
  \tightlist
  \item
    Tập dữ liệu gồm \(n\) mẫu, mỗi mẫu có \(d\) đặc trưng:
    \[X = \{x_1, x_2, \ldots, x_n\}, \quad x_i \in \mathbb{R}^d\]
  \item
    Nhãn tương ứng với các mẫu:
    \[Y = \{y_1, y_2, \ldots, y_n\}, \quad y_i \in \{G_1, G_2, \ldots, G_g\}\]
  \end{itemize}
\item
  \textbf{Đầu ra:} Nhóm \(G^*\) tối ưu cho một quan sát mới \(x\), xác
  định theo: \[G^* = \arg\max_{G} P(G | x)\] hoặc theo hàm phân biệt
  (discriminant function): \[G^* = \arg\max_{G} \delta_G(x)\]
\item
  \textbf{Mục tiêu của bài toán:} là xây dựng một hàm phân loại \(f\) để
  gán một quan sát \(x\) có \(d\) đặc trưng vào một trong \(g\) nhóm
  (classes) sao cho tối ưu được độ chính xác phân loại.

  Cụ thể, cần tìm một ánh xạ:
  \[f: \mathbb{R}^d \to \{G_1, G_2, \ldots, G_g\}\] sao cho với mỗi quan
  sát \(x\), nhóm dự đoán \(G^* = f(x)\) là nhóm có xác suất hậu nghiệm
  cao nhất hoặc hàm phân biệt tối đa.
\end{itemize}

\subsubsection{Cấu trúc chung
(Framework)}\label{cux1ea5u-truxfac-chung-framework}

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth,height=\textheight]{}
\caption{Mô hình cấu trúc chung của bài toán phân lớp}
\end{figure}

\subsubsection{Đóng góp}\label{ux111uxf3ng-guxf3p}

Một câu hỏi quan trọng là làm thế nào để xác định hàm phân biệt
(\(\delta_G(x)\)). Một trong những phương pháp phổ biến để giải quyết
vấn đề này là Phân tích Phân biệt Tuyến tính (LDA). Tương tự như Phân
tích Thành phần Chính (PCA), LDA chiếu toàn bộ tập dữ liệu lên một siêu
phẳng có số chiều nhỏ hơn, với mục tiêu phân tách dữ liệu một cách rõ
ràng hơn. Hàm phân biệt \(\delta_G(x)\) trong LDA có dạng tuyến tính,
cho phép phân tách các lớp bằng cách tìm ra hướng chiếu tối ưu nhất.

Vì thế, nghiên cứu này đóng góp vào lĩnh vực phân loại dữ liệu bằng cách
cung cấp một đánh giá toàn diện về Phân tích Phân biệt Tuyến tính (LDA)
từ cả góc độ lý thuyết và thực nghiệm. Cụ thể, các đóng góp chính của
nghiên cứu bao gồm:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Phân tch chi tiết về LDA trong bài toán phân loại:}

  Nghiên cứu trình bày một cách hệ thống nguyên lý hoạt động của LDA,
  bao gồm cách tiếp cận Fisher's Linear Discriminant (FLD) trong bài
  toán phân loại hai lớp và cách mở rộng phương pháp này cho nhiều lớp
  dữ liệu. Chúng tôi thảo luận các giả định quan trọng của LDA, bao gồm
  tính phân phối chuẩn của dữ liệu và tính đồng nhất của ma trận hiệp
  phương sai giữa các lớp.
\item
  \textbf{Đánh giá thực nghiệm hiệu suất của LDA:}

  Chúng tôi thực hiện các thí nghiệm trên các tập dữ liệu có nhãn để
  đánh giá hiệu suất phân loại của LDA trong các điều kiện khác nhau.
  Các chỉ số đánh giá như \ldots.. được sử dụng để định lượng hiệu quả
  của mô hình. Kết quả thực nghiệm cho thấy LDA có hiệu suất tốt khi dữ
  liệu tuân theo các giả định lý thuyết, nhưng có thể bị suy giảm khi vi
  phạm các giả định này.
\item
  \textbf{Phân tích ứng dụng thực tế của LDA:}

  Nghiên cứu và áp dụng ứng dụng thực tế của LDA trên tập dữ liệu tiêu
  chuẩn để đánh giá tính ứng dụng và hiệu suất của phương pháp này.
  Chúng tôi nhấn mạnh vai trò của LDA trong việc trích xuất đặc trưng và
  giảm chiều dữ liệu, đặc biệt trong các bài toán có số lượng đặc trưng
  lớn và yêu cầu tối ưu hóa tính toán.
\end{enumerate}

\pagebreak

\section{Các công trình nghiên cứu liên
quan}\label{cuxe1c-cuxf4ng-truxecnh-nghiuxean-cux1ee9u-liuxean-quan}

\subsection{Phân tích phân biệt tuyến tính (Linear Discriminant
Analysis)}\label{phuxe2n-tuxedch-phuxe2n-biux1ec7t-tuyux1ebfn-tuxednh-linear-discriminant-analysis}
\input{src/2_1.linear_discriminant_analysis}

\subsection{Phân biệt cực đại hợp lý (Maximum Likelihood
Discriminant)}\label{phuxe2n-biux1ec7t-cux1ef1c-ux111ux1ea1i-hux1ee3p-luxfd-maximum-likelihood-discriminant}
\input{src/2_2_MLD}

\subsection{Quy tắc phân biệt Bayes (Bayes Discriminant
Rule)}\label{quy-tux1eafc-phuxe2n-biux1ec7t-bayes-bayes-discriminant-rule}

\subsection{Quy tắc phân biệt tuyến tính của Fisher (Fisher's Linear
Discriminant
Rule)}\label{quy-tux1eafc-phuxe2n-biux1ec7t-tuyux1ebfn-tuxednh-cux1ee7a-fisher-fishers-linear-discriminant-rule}

\subsection{So sánh}\label{so-suxe1nh}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2188}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2750}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2562}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Phương pháp
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Nguyên lý
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Ưu điểm
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Hạn chế
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Phân biệt cực đại hợp lý (Maximum Likelihood Discriminant) & Ước lượng
tham số bằng phương pháp cực đại hợp lý (MLE). & Không cần thông tin
tiên nghiệm, dựa hoàn toàn vào dữ liệu quan sát. & Cần dữ liệu lớn để
ước lượng chính xác. \\
Quy tắc phân biệt Bayes (Bayes Discriminant Rule) & Áp dụng định lý
Bayes để tính xác suất hậu nghiệm. & Tận dụng được thông tin tiên
nghiệm, có thể cải thiện độ chính xác. & Cần biết xác suất tiên nghiệm,
nếu ước lượng sai sẽ ảnh hưởng đến kết quả. \\
Quy tắc phân biệt tuyến tính của Fisher (Fisher's Linear Discriminant) &
Tìm mặt phẳng phân tách tuyến tính tối ưu giữa các nhóm. & Không yêu cầu
giả định về phân phối, tính toán đơn giản. & Không hoạt động tốt nếu dữ
liệu phi tuyến. \\
\end{longtable}

\begin{itemize}
\tightlist
\item
  \textbf{Nhận xét:}

  \begin{itemize}
  \tightlist
  \item
    Nếu dữ liệu có phân phối chuẩn và đủ lớn, phương pháp phân biệt cực
    đại hợp lý sẽ được sử dụng để ước lượng tham số tối ưu.
  \item
    Nếu cần tận dụng thông tin tiên nghiệm, quy tắc phân biệt Bayes sẽ
    được sử dụng để tối ưu hóa phân loại theo xác suất hậu nghiệm.
  \item
    Nếu không muốn giả định về phân phối dữ liệu, quy tắc phân biệt
    tuyến tính của Fisher sẽ được sử dụng để tìm mặt phẳng phân tách
    tuyến tính tốt nhất.
  \end{itemize}
\item
  \textbf{Kết luận:} Quy tắc phân biệt tuyến tính của Fisher là một bước
  tiến giúp giảm yêu cầu về giả định phân phối dữ liệu, trong khi quy
  tắc phân biệt Bayes cải thiện phương pháp phân biệt cực đại hợp lý
  bằng cách sử dụng xác suất tiên nghiệm.
\end{itemize}


\section{Phương pháp: Linear Discriminant Analysis
(LDA)}\label{phux1b0ux1a1ng-phuxe1p-linear-discriminant-analysis-lda}

\subsection{LDA với hai lớp (Fisher's Discriminant
Rule)}\label{lda-vux1edbi-hai-lux1edbp-fishers-discriminant-rule}

\subsubsection{Bài toán LDA hai
lớp}\label{buxe0i-touxe1n-lda-hai-lux1edbp}

Giả sử có \(N\) điểm dữ liệu
\(\mathbf{x}_1, \mathbf{x}_2, …, \mathbf{x}_N\) trong không gian
\(d\)-chiều, với \(N_1\) điểm thuộc lớp thứ nhất và \(N_2 = N - N_1\)
điểm thuộc lớp thứ hai.

Tập chỉ số các điểm thuộc từng lớp:

\begin{itemize}
\tightlist
\item
  \(\mathbf{C}_1 = \{ n \; | \; 1 \leq n \leq N_1 \}\): là tập chỉ số
  của các điểm thuộc lớp thứ nhất.
\item
  \(\mathbf{C}_2 = \{ m \; | \; N_1 + 1 \leq m \leq N \}\): là tập chỉ
  số của các điểm thuộc lớp thứ hai.
\end{itemize}

Mục tiêu là tìm một vector hệ số \(\mathbf{w}\) để chiếu dữ liệu lên một
trục sao cho khả năng phân biệt giữa các lớp là tốt nhất. Giá trị sau
khi chiếu của mỗi điểm là:

\[y_n = \mathbf{w}^T \mathbf{x}_n, \quad 1 \leq n \leq N\]

\begin{figure}
\centering
\includegraphics{./assets/projection.png}
\caption{Chiếu tập dữ liệu lên các đường thẳng khác nhau thì khả năng
phân biệt giữa các lớp cũng khác nhau.}
\end{figure}

Ta thấy phép chiếu bên trái đã có sự tối đa hóa về việc phân chia dữ
liệu độc lập nhau. Để có thể tìm được một vector chiếu tốt nhất, ta cần
xác định một quy tắc (rule) để phân chia dữ liệu.

Như đã phân tích trong Mục \href{}{2}, hai lớp được xem là phân biệt tốt
(discriminative) nếu khoảng cách giữa chúng lớn, tức là phương sai giữa
lớp (between-class variance) lớn, và dữ liệu trong từng lớp có mức độ
đồng nhất cao, tức là phương sai trong lớp (within-class variance) nhỏ.

Từ đó, phương pháp Linear Discriminant Analysis (LDA) có thể được hiểu
là một thuật toán nhằm tìm một phép chiếu tối ưu sao cho tỷ lệ giữa
phương sai giữa lớp và phương sai trong lớp đạt giá trị lớn nhất. Mục
tiêu của phương pháp này là tối ưu hóa khả năng phân biệt giữa các lớp
trong không gian đặc trưng.

Để cực đại hóa tỷ lệ giữa phương sai giữa lớp và phương sai trong lớp,
cần xác định một hàm mục tiêu phù hợp. Hàm mục tiêu này được xây dựng
dựa trên việc tìm cực trị của tỷ số giữa phương sai giữa lớp và phương
sai trong lớp nhằm mô hình hóa và tối ưu hóa quá trình phân tách giữa
các lớp.

\subsubsection{Xây dựng hàm mục
tiêu}\label{xuxe2y-dux1ef1ng-huxe0m-mux1ee5c-tiuxeau}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Phương sai giữa lớp (Between-Class Variance)}

  Gọi \(\mu_i\) là vector trung bình của các mẫu thuộc lớp
  \(\mathbf{C}_i\) trong không gian ban đầu \(\mathbb{R}^d\), được tính
  bằng:

  \[\mu_i = \frac{1}{N_i} \sum_{x_n \in \mathbf{C}_i} \mathbf{x}_n, \quad i = 1,2\]

  trong đó:

  \begin{itemize}
  \tightlist
  \item
    \(N_i\) là số lượng điểm dữ liệu thuộc lớp \(\mathbf{C}_i\).
  \item
    \(\mathbf{x}_n\) là một điểm dữ liệu thuộc lớp \(\mathbf{C}_i\).
  \item
    \(\boldsymbol{\mu}_i\) là vector trung bình của lớp \(\mathbf{C}_i\)
    trong không gian đặc trưng \(\mathbb{R}^d\).
  \end{itemize}

  Sau khi dữ liệu được chiếu lên phương \(\mathbf{w}\), trung bình mẫu
  trong không gian mới (không gian 1 chiều) được ký hiệu là \(m_i\):
  \[m_i = \frac{1}{N_i} \sum_{x_n \in \mathbf{C}_i} y_n\] Thay
  \(y_n = \mathbf{w}^T \mathbf{x}_n\), ta có:
  \[m_i = \frac{1}{N_i} \sum_{x_n \in \mathbf{C}_i} \mathbf{w}^T \mathbf{x}_n\]
  Sử dụng tính chất tuyến tính của tích vô hướng:
  \[m_i = \mathbf{w}^T \left( \frac{1}{N_i} \sum_{x_n \in \mathbf{C}_i} \mathbf{x}_n \right) = \mathbf{w}^T \mu_i\]
  \[\implies (m_1 - m_2) = w^T\mu_1 - w^T\mu_2 = w^T(\mu_1-\mu_2)\]

  Xét đẳng thức:
  \[(a^T b)^2 = (a^T b)(a^T b) = a^T b b^T a \quad (3.1)\] với \(a, b\)
  là hai véc tơ cùng chiều bất kỳ.

  Khi đó, phương sai giữa lớp (between-class variance) là:
  \[(m_1 - m_2)^2 =[w^T(\mu_1-\mu_2)]^2= w^T(\mu_1-\mu_2)(\mu_1-\mu_2)^Tw\]
  (Áp dụng đẳng thức ma trận \href{}{3.1})

  Đặt \(S_B=(\mu_1-\mu_2)(\mu_1-\mu_2)^T\), khi đó phương sai giữa lớp
  trở thành: \[(m_1 - m_2)^2 = \mathbf{w}^TS_B\mathbf{w}\]
\item
  \textbf{Phương sai trong lớp (Within-Class Variance)}

  Phương sai trong lớp (within-class variance) được định nghĩa là tổng
  phương sai của hai lớp \(s_1^2 + s_2^2\).

  Các phương sai trong lớp (within-class variances) được định nghĩa là:

  \[
   \begin{aligned}
   s_k^2 & =\sum_{n \in \mathbf{C}_1}\left(y_n-m_k\right)^2, \quad k=1,2, \ldots \\
   & =\sum_{n \in \mathbf{C}_1}\left(\mathbf{w}^T \mathbf{x}-\mathbf{w}^T \mu_k\right)^2 \\
   & =\sum_{n \in \mathbf{C}_1}\left(\mathbf{w}^T(\mathbf{x}_n-\mu_k)\right)^2 \\
   \end{aligned}
   \]

  \begin{itemize}
  \tightlist
  \item
    \textbf{Lưu ý:} Các phương sai trong lớp ở đây không được tính theo
    cách trung bình như phương sai thông thường. Mức độ ảnh hưởng của
    mỗi phương sai trong lớp phụ thuộc vào số lượng điểm dữ liệu trong
    lớp đó. Lớp có nhiều điểm dữ liệu hơn sẽ đóng góp nhiều hơn vào tổng
    phương sai trong lớp. Do đó, thay vì lấy trung bình, ta giữ nguyên
    tổng phương sai trong lớp bằng cách nhân phương sai với số lượng
    điểm dữ liệu trong lớp. Ta có thể hiểu phương sai trong lớp thực
    chất là phương sai thông thường nhân với số lượng điểm dữ liệu trong
    lớp đó.
  \end{itemize}

  Vậy phương sai trong lớp khi đó:

  \[\begin{aligned}
   s_1^2+s_2^2 & =\sum_{k=1}^2\sum_{n \in \mathbf{C}_1}\left(y_n-m_k\right)^2, \quad k=1,2, \ldots \\
   & =\sum_{k=1}^2\sum_{n \in \mathbf{C}_1}\left(\mathbf{w}^T \mathbf{x}-\mathbf{w}^T \mu_k\right)^2 \\
   & =\sum_{k=1}^2\sum_{n \in \mathbf{C}_1}\left(\mathbf{w}^T(\mathbf{x}_n-\mu_k)\right)^2 \\
   \end{aligned}
   \] Áp dụng đẳng thức ma trận \href{}{3.1} \[\begin{aligned}
   s_1^2+s_2^2 
   & =\sum_{k=1}^2\sum_{n \in 
   \mathbf{C}_1}\left(\mathbf{w}^T(\mathbf{x}_n-\mu_k)(\mathbf{x}_n-\mu_k)^T\mathbf{w} \right) \\
   & =\mathbf{w}^T\sum_{k=1}^2\sum_{n \in \mathbf{C}_1}\left((\mathbf{x}_n-\mu_k)(\mathbf{x}_n-\mu_k)^T\right)\mathbf{w}  \\
   \end{aligned}\]

  Đặt
  \(S_w = \sum_{k=1}^2\sum_{n \in \mathbf{C}_1}(\mathbf{x}_n-\mu_k)(\mathbf{x}_n-\mu_k)^T\)
  thì phương sai trong lớp sẽ trở thành:
  \[s_1^2 + s_2^2 = \mathbf{w} ^TS_w\mathbf{w}\]
\item
  \textbf{Hàm mục tiêu }

  LDA là thuật toán đi tìm giá trị lớn nhất của hàm mục tiêu:
  \[J(\mathbf{w}) = \frac{(m_1 - m_2)^2}{s_1^2+s_2^2} \]

  Bây giờ, bài toán tối ưu cho LDA trở thành:
  \[J(\mathbf{w}) =\frac{\mathbf{w}^TS_B\mathbf{w}}{\mathbf{w} ^TS_w\mathbf{w}} \quad \text{và} \quad \mathbf{w} = argmax \frac{\mathbf{w}^TS_B\mathbf{w}}{\mathbf{w} ^TS_w\mathbf{w}}\]

  với:

  \begin{itemize}
  \tightlist
  \item
    \(S_B=(\mu_1-\mu_2)(\mu_1-\mu_2)^T\)
  \item
    \(S_w = \sum_{k=1}^2\sum_{n \in \mathbf{C}_1}(\mathbf{x}_n-\mu_k)(\mathbf{x}_n-\mu_k)^T\)
  \end{itemize}
\end{enumerate}

\subsubsection{Bài toán tối ưu
hoá}\label{buxe0i-touxe1n-tux1ed1i-ux1b0u-houxe1}

Để tìm giá trị lớn nhất của hàm mục tiêu, ta thực hiện đạo hàm \(J(w)\)
và cho kết quả bằng \(0\). \[
\begin{aligned}
&  \frac{\partial J(\mathbf{w})}{\partial \mathbf{w}}=\frac{\partial}{\partial \mathbf{w}}\left(\frac{\mathbf{w}^T S_B \mathbf{w}}{\mathbf{w}^T S_{\mathbf{w}} \mathbf{w}}\right)=0 \\
& \Leftrightarrow\left(\mathbf{w}^T S_{\mathbf{w}} \mathbf{w}\right) \frac{d}{d \mathbf{w}}\left(\mathbf{w}^T S_B \mathbf{w}\right)-\left(\mathbf{w}^T S_B \mathbf{w}\right) \frac{d}{d \mathbf{w}}\left(\mathbf{w}^T S_{\mathbf{w}} \mathbf{w}\right)=0 \\
& \Leftrightarrow\left(\mathbf{w}^T S_{\mathbf{w}} \mathbf{w}\right) 2 S_B \mathbf{w}-\left(\mathbf{w}^T S_B \mathbf{w}\right) 2 S_{\mathbf{w}} \mathbf{w}=0 \\
& \Leftrightarrow\left(\frac{\mathbf{w}^T S_{\mathbf{w}} \mathbf{w}}{\mathbf{w}^T S_{\mathbf{w}} \mathbf{w}}\right) S_B \mathbf{w}-\left(\frac{\mathbf{w}^T S_B \mathbf{w}}{\mathbf{w}^T S_{\mathbf{w}} \mathbf{w}}\right) S_{\mathbf{w}} \mathbf{w}=0 \\
& \Leftrightarrow S_B \mathbf{w}-J(\mathbf{w}) S_{\mathbf{w}} \mathbf{w}=0 \\
& \Leftrightarrow S_{\mathbf{w}}^{-1} S_B \mathbf{w}-J(\mathbf{w}) \mathbf{w}=0 \\
& \Leftrightarrow S_{\mathbf{w}}^{-1} S_B \mathbf{w}=J(\mathbf{w}) \mathbf{w} \quad (3.2)\\
\end{aligned}
\]

Vì \(J(\mathbf{w})\) là một số vô hướng, nên suy ra \(\mathbf{w}\) phải
là một vector riêng của ma trận \(\mathbf{S}_W^{-1} \mathbf{S}_B\) ứng
với một trị riêng nào đó.

Hơn nữa, giá trị của trị riêng này chính bằng \(J(\mathbf{w})\). Do đó,
để hàm mục tiêu đạt giá trị lớn nhất, ta cần chọn trị riêng lớn nhất của
\(\mathbf{S}_W^{-1} \mathbf{S}_B\). Điều này có nghĩa là \(\mathbf{w}\)
sẽ là vector riêng ứng với trị riêng lớn nhất của
\(\mathbf{S}_W^{-1} \mathbf{S}_B\).

Vì \(\mathbf{w}\) là nghiệm của
\(\mathbf{w} = \arg\max \frac{\mathbf{w}^T S_B \mathbf{w}}{\mathbf{w}^T S_w \mathbf{w}}\),
nên bất kỳ bội số \(k \mathbf{w}\) (với \(k \neq 0\)) cũng là nghiệm.
Điều này là do giá trị hàm mục tiêu chỉ phụ thuộc vào hướng của
\(\mathbf{w}\), không phụ thuộc vào độ lớn của nó. Do đó, mọi nghiệm
thuộc về một đường thẳng trong không gian, không phải là một điểm duy
nhất.

\textbf{Chứng minh:}

Giả sử \(\mathbf{w}^*\) là nghiệm tối ưu, tức là:
\[J(\mathbf{w^*}) =\frac{\mathbf{w^*}^T S_B \mathbf{w^*}}{\mathbf{w^*}^T S_W \mathbf{w^*}} = \max_{\mathbf{w}} \frac{\mathbf{w}^T S_B \mathbf{w}}{\mathbf{w}^T S_W \mathbf{w}}\]

Xét bội số \(k \mathbf{w}^*\) với \(k \neq 0\), ta có: \[
\begin{aligned}
J(k\mathbf{w^*}) =
\frac{(k\mathbf{w}^*)^T S_B (k\mathbf{w}^*)}{(k\mathbf{w}^*)^T S_W (k\mathbf{w}^*)}  
= \frac{k^2 (\mathbf{w}^*)^T S_B (\mathbf{w}^*)}{k^2 (\mathbf{w}^*)^T S_W (\mathbf{w}^*)} 
= \frac{(\mathbf{w}^*)^T S_B (\mathbf{w}^*)}{(\mathbf{w}^*)^T S_W (\mathbf{w}^*)} = J(\mathbf{w})
\end{aligned} \quad (3.3)
\]

Ở đây ta thấy, khi giải bài toán LDA, hàm mục tiêu \(J(\mathbf{w})\) chỉ
phụ thuộc vào hướng của \(\mathbf{w}\) chứ không phụ thuộc vào độ lớn
của nó. Điều này có nghĩa là nếu \(\mathbf{w}\) là nghiệm, thì mọi bội
số \(k\mathbf{w}\) (với \(k \neq 0\)) cũng là nghiệm.

Để ``đóng băng'' độ lớn của \(\mathbf{w}\) hay chuẩn hóa độ lớn của nó
mà không làm thay đổi hướng, ta có thể đặt thêm điều kiện:
\[(\boldsymbol{\mu}_1 - \boldsymbol{\mu}_2)^T \mathbf{w} = J(\mathbf{w}) = L \quad (3.4)\]

với \(L\) là trị riêng lớn nhất của \(\mathbf{S}_W^{-1}\mathbf{S}_B\).

Điều này đảm bảo rằng khoảng cách giữa trung bình các lớp sau khi chiếu
(tính bằng
\(m_1 - m_2 = (\boldsymbol{\mu}_1 - \boldsymbol{\mu}_2)^T \mathbf{w}\))
đạt đúng mức tối ưu (tương đương với giá trị riêng lớn nhất \(L\)). Qua
đó, ta có được một hướng \(\mathbf{w}\) đã được chuẩn hóa phù hợp với
mục tiêu phân biệt tối ưu.

Bây giờ, chúng ta đi giải bài toán trị riêng \((3.2)\) với điều kiện
\((3.4)\).

Với
\(\mathbf{S}_B = (\boldsymbol{\mu}_1 - \boldsymbol{\mu}_2)(\boldsymbol{\mu}_1 - \boldsymbol{\mu}_2)^T\),
thay vào \((*)\):
\[L\mathbf{w}= J(\mathbf{w} ) \mathbf{w} =\mathbf{S}_W^{-1} \bigl[(\boldsymbol{\mu}_1 - \boldsymbol{\mu}_2)(\boldsymbol{\mu}_1 - \boldsymbol{\mu}_2)^T \bigr] \mathbf{w}  = \mathbf{S}_W^{-1} (\boldsymbol{\mu}_1 - \boldsymbol{\mu}_2)(\boldsymbol{\mu}_1 - \boldsymbol{\mu}_2)^T \mathbf{w}\]
mà \(L =(\boldsymbol{\mu}_1 - \boldsymbol{\mu}_2)^T \mathbf{w}\) nên:

\[
\begin{aligned}
&L \mathbf{w} = \mathbf{S}_W^{-1} (\boldsymbol{\mu}_1 - \boldsymbol{\mu}_2)(\boldsymbol{\mu}_1 - \boldsymbol{\mu}_2)^T \mathbf{w} \\
\Leftrightarrow \quad & L \mathbf{w} = L \mathbf{S}_W^{-1} (\boldsymbol{\mu}_1 - \boldsymbol{\mu}_2) \\
\Leftrightarrow \quad & \mathbf{w} = \mathbf{S}_W^{-1} (\boldsymbol{\mu}_1 - \boldsymbol{\mu}_2)
\end{aligned}
\]

Do \(\mathbf{w}\) là nghiệm, nên bất kỳ bội số \(k \mathbf{w}\) (với
\(k \neq 0\)) cũng là nghiệm (kết quả chứng minh \href{}{3.3}), nên
nghiệm tổng quát của \(\mathbf{w}\) là:
\[\mathbf{w} = \alpha\mathbf{S}_W^{-1} (\boldsymbol{\mu}_1 - \boldsymbol{\mu}_2), \quad \text{ với } \alpha \not = 0 \text{ bất kỳ}\]

\pagebreak

\subsubsection{Phương pháp phân
lớp}\label{phux1b0ux1a1ng-phuxe1p-phuxe2n-lux1edbp}

\pagebreak

\subsection{LDA với đa lớp}\label{lda-vux1edbi-ux111a-lux1edbp}

\section{Cài đặt và thực
nghiệm}\label{cuxe0i-ux111ux1eb7t-vuxe0-thux1ef1c-nghiux1ec7m}

\section{Kết luận và hướng phát
triển}\label{kux1ebft-luux1eadn-vuxe0-hux1b0ux1edbng-phuxe1t-triux1ec3n}

\section{Tham khảo}\label{tham-khux1ea3o}

\end{document}
